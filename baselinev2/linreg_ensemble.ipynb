{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LinReg ensemble instad of simple mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since 4 models were trained on 4 folds, there is no proper validation data for ensemble coefficients creation. Couple of possible workaraunds:\n",
    "1. Fit ensemble coefficients on all 4 folds simultaneously (on each fold 3 correspondent models will be better, but overall coefficients may be fair)\n",
    "1. Fit coefs on the unused birdclef23 data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/yaz/birdclef24/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import os\n",
    "import gc\n",
    "import sys\n",
    "import cv2\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from glob import glob\n",
    "from tqdm.auto import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import librosa\n",
    "import scipy as sci\n",
    "import timm\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torchvision.models import efficientnet\n",
    "\n",
    "import pytorch_lightning as pl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torchvision\n",
    "import torch\n",
    "import pytorch_lightning as pl\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "import math\n",
    "import librosa\n",
    "import cv2\n",
    "import torch.nn as nn\n",
    "from metric import score\n",
    "import albumentations as A\n",
    "import numpy as np\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import torchaudio\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "    # == global config ==\n",
    "    SEED = 42 # random seed\n",
    "    DEVICE = 'cuda'  # device to be used\n",
    "    MIXED_PRECISION = False  # whether to use mixed-16 precision\n",
    "    # OUTPUT_DIR = '/kaggle/working/'  # output folder\n",
    "    OUTPUT_DIR = '/data/yaz/birdclef24/out/'  # output folder\n",
    "    CKPT_ROOT = '/data/yaz/birdclef24/baselinev2/out/'\n",
    "\n",
    "    use_cache = False\n",
    "    \n",
    "    # == data config ==\n",
    "    # DATA_ROOT = '/kaggle/input/birdclef-2024'  # root folder\n",
    "    DATA_ROOT = '/data/yaz/birdclef24/data'  # root folder\n",
    "    # PREPROCESSED_DATA_ROOT = '/kaggle/input/birdclef24-spectrograms-via-cupy'\n",
    "    # PREPROCESSED_DATA_ROOT = '/data/yaz/birdclef24/data/specs'\n",
    "    # LOAD_DATA = True  # whether to load data from pre-processed dataset\n",
    "\n",
    "    image_size = 256\n",
    "\n",
    "    SR = 32000  # sample rate\n",
    "    mel_spec_params = {\n",
    "        \"sample_rate\": 32000,\n",
    "        \"n_mels\": 128,\n",
    "        \"f_min\": 20,\n",
    "        \"f_max\": 16000,\n",
    "        \"n_fft\": 2048,\n",
    "        \"hop_length\": 512,\n",
    "        \"normalized\": True,\n",
    "        \"center\" : True,\n",
    "        \"pad_mode\" : \"constant\",\n",
    "        \"norm\" : \"slaney\",\n",
    "        \"onesided\" : True,\n",
    "        \"mel_scale\" : \"slaney\"\n",
    "    }\n",
    "\n",
    "    top_db = 80 \n",
    "\n",
    "    train_period = 5\n",
    "    val_period = 5\n",
    "    secondary_coef = 1.0\n",
    "    train_duration = train_period * mel_spec_params[\"sample_rate\"]\n",
    "    val_duration = val_period * mel_spec_params[\"sample_rate\"]\n",
    "\n",
    "    # == model config ==\n",
    "    MODEL_TYPE = 'efficientnet_b0'  # model type\n",
    "    \n",
    "    # == dataset config ==\n",
    "    # BATCH_SIZE = 128  # batch size of each step\n",
    "    BATCH_SIZE = 256 # batch size of each step\n",
    "    N_WORKERS = 4  # number of workers\n",
    "    # N_WORKERS = 0  # number of workers\n",
    "    \n",
    "    # == AUG ==\n",
    "    USE_XYMASKING = True  # whether use XYMasking\n",
    "    \n",
    "    # == training config ==\n",
    "    N_FOLDS = 4  # n fold\n",
    "    EPOCHS = 30  # max epochs\n",
    "    LR = 1e-3  # learning rate\n",
    "    WEIGHT_DECAY = 1e-5  # weight decay of optimizer\n",
    "\n",
    "    \n",
    "    # == other config ==\n",
    "    VISUALIZE = False # whether to visualize data and batch\n",
    "\n",
    "\n",
    "config = Config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fix seed\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "42"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('fix seed')\n",
    "pl.seed_everything(config.SEED, workers=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(config.DATA_ROOT + '/train_metadata.csv')\n",
    "df[\"path\"] = config.DATA_ROOT + \"/train_audio/\" + df[\"filename\"]\n",
    "df[\"rating\"] = np.clip(df[\"rating\"] / df[\"rating\"].max(), 0.1, 1.0)\n",
    "\n",
    "skf = StratifiedKFold(n_splits=config.N_FOLDS, random_state=config.SEED, shuffle=True)\n",
    "df['fold'] = -1\n",
    "for ifold, (train_idx, val_idx) in enumerate(skf.split(X=df, y=df[\"primary_label\"].values)):\n",
    "    df.loc[val_idx, 'fold'] = ifold\n",
    "\n",
    "sub = pd.read_csv(config.DATA_ROOT + \"/sample_submission.csv\")\n",
    "target_columns = sub.columns.tolist()[1:]\n",
    "label_list = target_columns\n",
    "num_classes = len(target_columns)\n",
    "bird2id = {b: i for i, b in enumerate(target_columns)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_melspec(X, eps=1e-6):\n",
    "    mean = X.mean((1, 2), keepdim=True)\n",
    "    std = X.std((1, 2), keepdim=True)\n",
    "    Xstd = (X - mean) / (std + eps)\n",
    "\n",
    "    norm_min, norm_max = (\n",
    "        Xstd.min(-1)[0].min(-1)[0],\n",
    "        Xstd.max(-1)[0].max(-1)[0],\n",
    "    )\n",
    "    fix_ind = (norm_max - norm_min) > eps * torch.ones_like(\n",
    "        (norm_max - norm_min)\n",
    "    )\n",
    "    V = torch.zeros_like(Xstd)\n",
    "    if fix_ind.sum():\n",
    "        V_fix = Xstd[fix_ind]\n",
    "        norm_max_fix = norm_max[fix_ind, None, None]\n",
    "        norm_min_fix = norm_min[fix_ind, None, None]\n",
    "        V_fix = torch.max(\n",
    "            torch.min(V_fix, norm_max_fix),\n",
    "            norm_min_fix,\n",
    "        )\n",
    "        V_fix = (V_fix - norm_min_fix) / (norm_max_fix - norm_min_fix)\n",
    "        V[fix_ind] = V_fix\n",
    "    return V\n",
    "\n",
    "\n",
    "def read_wav(path):\n",
    "    wav, org_sr = torchaudio.load(path, normalize=True)\n",
    "    wav = torchaudio.functional.resample(wav, orig_freq=org_sr, new_freq=config.SR)\n",
    "    return wav\n",
    "\n",
    "\n",
    "def crop_start_wav(wav, duration_):\n",
    "    while wav.size(-1) < duration_:\n",
    "        wav = torch.cat([wav, wav], dim=1)\n",
    "    wav = wav[:, :duration_]\n",
    "    return wav"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "transforms_val = A.Compose([\n",
    "    A.Resize(config.image_size, config.image_size),\n",
    "    A.Normalize()\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 24459/24459 [00:08<00:00, 2801.99it/s]\n"
     ]
    }
   ],
   "source": [
    "all_waves = []\n",
    "all_targets = []\n",
    "\n",
    "# https://www.kaggle.com/code/markwijkhuizen/birdclef-2024-efficientvit-inference\n",
    "if len(glob(f'{config.DATA_ROOT}/test_soundscapes/*.ogg')) > 0:\n",
    "    ogg_file_paths = glob(f'{config.DATA_ROOT}/test_soundscapes/*.ogg')\n",
    "else:\n",
    "    ogg_file_paths = sorted(glob(f'{config.DATA_ROOT}/train_audio/*/*.ogg'))\n",
    "\n",
    "for i, file_path in tqdm(enumerate(ogg_file_paths), total=len(ogg_file_paths)):\n",
    "\n",
    "    if i % 100 != 0:\n",
    "        continue\n",
    "\n",
    "    bird = file_path.split(\"/\")[-2]\n",
    "    target = bird2id[bird]\n",
    "\n",
    "    row_id = re.search(r'/([^/]+)\\.ogg$', file_path).group(1)  # filename\n",
    "    audio_data = read_wav(file_path)\n",
    "\n",
    "    SR = config.SR\n",
    "    SEGMENT_DURATION = 5  # seconds\n",
    "    SEGMENT_SAMPLES = SR * SEGMENT_DURATION\n",
    "\n",
    "    # Iterate over the segments\n",
    "    for j in range(48):\n",
    "        start_idx = SR * SEGMENT_DURATION * j\n",
    "        end_idx = SR * SEGMENT_DURATION * (j + 1)\n",
    "        wave = audio_data[:, start_idx:end_idx]\n",
    "        \n",
    "        # Check if the wave is shorter than the required length\n",
    "        if wave.size(1) < SEGMENT_SAMPLES:\n",
    "            # Pad the wave with zeros\n",
    "            padding = SEGMENT_SAMPLES - wave.size(1)\n",
    "            wave = torch.nn.functional.pad(wave, (0, padding), \"constant\", 0)\n",
    "\n",
    "        all_waves.append(wave) \n",
    "        all_targets.append(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BirdDatasetInference(torch.utils.data.Dataset):\n",
    "    def __init__(self, wavs, transform=None, add_secondary_labels=False, mode=None, use_cache=config.use_cache):\n",
    "        self.wavs = wavs\n",
    "        self.processed = [False for _ in range(len(self.wavs))]\n",
    "        self.bird2id = bird2id\n",
    "        self.num_classes = num_classes\n",
    "        self.secondary_coef = config.secondary_coef\n",
    "        self.add_secondary_labels = add_secondary_labels\n",
    "        self.mel_transform = torchaudio.transforms.MelSpectrogram(**config.mel_spec_params)\n",
    "        self.db_transform = torchaudio.transforms.AmplitudeToDB(stype='power', top_db=config.top_db)\n",
    "        self.transform = transform\n",
    "        self.mode = mode\n",
    "        self.use_cache = use_cache\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.wavs)\n",
    "\n",
    "    def prepare_spec(self, wav):\n",
    "        mel_spectrogram = normalize_melspec(self.db_transform(self.mel_transform(wav)))\n",
    "        mel_spectrogram = mel_spectrogram * 255\n",
    "        mel_spectrogram = mel_spectrogram.expand(3, -1, -1).permute(1, 2, 0).numpy()\n",
    "        return mel_spectrogram\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        if not self.processed[idx]:\n",
    "            spec = self.prepare_spec(self.wavs[idx])\n",
    "\n",
    "            if self.transform is not None:\n",
    "                res = self.transform(image=spec)\n",
    "                spec = res['image'].astype(np.float32)\n",
    "            else:\n",
    "                spec = spec.astype(np.float32)\n",
    "\n",
    "            spec = spec.transpose(2, 0, 1)\n",
    "\n",
    "            if self.use_cache:\n",
    "                self.wavs[idx] = spec\n",
    "                self.processed[idx] = True\n",
    "        else:\n",
    "            spec = self.wavs[idx]\n",
    "\n",
    "        return {\"spec\": spec}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EffNet(nn.Module):\n",
    "    def __init__(self, model_name=config.MODEL_TYPE, num_classes=None) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.model = timm.create_model(\n",
    "            model_name, \n",
    "            pretrained=True, \n",
    "            in_chans=3, \n",
    "            num_classes=num_classes\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # [B, W, H] -> [B, 1, W, H]\n",
    "        # x = x.unsqueeze(1)\n",
    "        x = self.model(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FocalLossBCE():\n",
    "    pass\n",
    "\n",
    "class BirdModel(pl.LightningModule):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        # == backbone ==\n",
    "        self.backbone = EffNet(num_classes=num_classes)\n",
    "        \n",
    "        # == loss function ==\n",
    "        # self.loss_fn = nn.CrossEntropyLoss()\n",
    "        self.loss_fn = FocalLossBCE() \n",
    "        \n",
    "        # == record ==\n",
    "        self.validation_step_outputs = []\n",
    "        \n",
    "    def forward(self, images):\n",
    "        return self.backbone(images)\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        \n",
    "        # == define optimizer ==\n",
    "        model_optimizer = torch.optim.Adam(\n",
    "            filter(lambda p: p.requires_grad, self.parameters()),\n",
    "            lr=config.LR,\n",
    "            weight_decay=config.WEIGHT_DECAY\n",
    "        )\n",
    "        \n",
    "        lr_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "            model_optimizer,\n",
    "            T_max=config.EPOCHS,\n",
    "            eta_min=1e-6,\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'optimizer': model_optimizer,\n",
    "            'lr_scheduler': {\n",
    "                'scheduler': lr_scheduler,\n",
    "                'interval': 'epoch',\n",
    "                'monitor': 'val_loss',\n",
    "                'frequency': 1\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    @staticmethod\n",
    "    def mixup(data, targets, alpha):\n",
    "        indices = torch.randperm(data.size(0))\n",
    "        data2 = data[indices]\n",
    "        targets2 = targets[indices]\n",
    "\n",
    "        lam = torch.tensor([np.random.beta(alpha, alpha)], device=data.device)\n",
    "        data = data * lam + data2 * (1 - lam)\n",
    "        targets = targets * lam + targets2 * (1 - lam)\n",
    "\n",
    "        return data, targets\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        \n",
    "        # == obtain input and target ==\n",
    "        image, target = batch[\"spec\"], batch[\"target\"]\n",
    "        image, target = self.mixup(image, target, 0.5)\n",
    "        \n",
    "        # == pred ==\n",
    "        y_pred = self(image)\n",
    "        \n",
    "        # == compute loss ==\n",
    "        train_loss = self.loss_fn(y_pred, target)\n",
    "        \n",
    "        # == record ==\n",
    "        self.log('train_loss', train_loss, prog_bar=True, on_step=True, on_epoch=True)\n",
    "        \n",
    "        return train_loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        \n",
    "        # == obtain input and target ==\n",
    "        image, target = batch['spec'], batch['target']\n",
    "        image = image.to(self.device)\n",
    "        target = target.to(self.device)\n",
    "        \n",
    "        # == pred ==\n",
    "        with torch.no_grad():\n",
    "            y_pred = self(image)\n",
    "            \n",
    "        self.validation_step_outputs.append({\"logits\": y_pred, \"targets\": target})\n",
    "        \n",
    "    def train_dataloader(self):\n",
    "        return self._train_dataloader\n",
    "\n",
    "    def validation_dataloader(self):\n",
    "        return self._validation_dataloader\n",
    "    \n",
    "    def on_validation_epoch_end(self):\n",
    "        \n",
    "        # = merge batch data =\n",
    "        outputs = self.validation_step_outputs\n",
    "        \n",
    "        output_val = nn.Softmax(dim=1)(torch.cat([x['logits'] for x in outputs], dim=0)).cpu().detach()\n",
    "        target_val = torch.cat([x['targets'] for x in outputs], dim=0).cpu().detach()\n",
    "        \n",
    "        # = compute validation loss =\n",
    "        val_loss = self.loss_fn(output_val, target_val)\n",
    "        \n",
    "        # target to one-hot\n",
    "        # target_val = torch.nn.functional.one_hot(target_val, num_classes)\n",
    "        \n",
    "        # = val with ROC AUC =\n",
    "        gt_df = pd.DataFrame(target_val.numpy().astype(np.float32), columns=label_list)\n",
    "        pred_df = pd.DataFrame(output_val.numpy().astype(np.float32), columns=label_list)\n",
    "        \n",
    "        gt_df['id'] = [f'id_{i}' for i in range(len(gt_df))]\n",
    "        pred_df['id'] = [f'id_{i}' for i in range(len(pred_df))]\n",
    "        \n",
    "        val_score = score(gt_df, pred_df, row_id_column_name='id')\n",
    "        \n",
    "        self.log(\"val_score\", val_score, on_epoch=True)\n",
    "        self.log(\"val_loss\", val_loss, on_epoch=True)\n",
    "        \n",
    "        # clear validation outputs\n",
    "        self.validation_step_outputs = list()\n",
    "        \n",
    "        return {'val_loss': val_loss, 'val_score': val_score}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(data_loader, onnx_model):\n",
    "    pred = []\n",
    "    for batch in tqdm(data_loader):\n",
    "        with torch.no_grad():\n",
    "            x = batch['spec']\n",
    "            n_pad = 0\n",
    "            \n",
    "            # == make sure the batch_size equal to setting\n",
    "            if x.shape[0] < config.BATCH_SIZE:\n",
    "                n_pad = config.BATCH_SIZE - x.shape[0]\n",
    "                zero_tensor = torch.zeros((n_pad, 3, 256, 256))\n",
    "                x = torch.cat([x, zero_tensor], dim=0)\n",
    "            \n",
    "            outputs = onnx_model.run(output_names, {input_names[0]: x.numpy()})[0]\n",
    "            outputs = sci.special.softmax(outputs[:config.BATCH_SIZE-n_pad, ...], axis=1)\n",
    "        pred.append(outputs)\n",
    "    \n",
    "    return np.concatenate(pred, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnx\n",
    "import onnxruntime as ort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "onnx_ckpt_list = [\n",
    "    f\"/data/yaz/birdclef24/baselinev2/fold_{f}.onnx\" for f in range(4)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/yaz/birdclef24/.venv/lib/python3.10/site-packages/torchaudio/transforms/_transforms.py:580: UserWarning: Argument 'onesided' has been deprecated and has no influence on the behavior of this module.\n",
      "  warnings.warn(\n",
      "  0%|          | 0/46 [00:10<?, ?it/s]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'output_names' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 20\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;66;03m# == create dataset & dataloader ==\u001b[39;00m\n\u001b[1;32m     12\u001b[0m     test_loader \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mDataLoader(\n\u001b[1;32m     13\u001b[0m         test_dataset,\n\u001b[1;32m     14\u001b[0m         batch_size\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mBATCH_SIZE,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     17\u001b[0m         drop_last\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m     18\u001b[0m     )\n\u001b[0;32m---> 20\u001b[0m     predictions\u001b[38;5;241m.\u001b[39mappend(\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43monnx_session\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     21\u001b[0m     gc\u001b[38;5;241m.\u001b[39mcollect()\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# predictions = np.mean(predictions, axis=0)\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[19], line 14\u001b[0m, in \u001b[0;36mpredict\u001b[0;34m(data_loader, onnx_model)\u001b[0m\n\u001b[1;32m     11\u001b[0m         zero_tensor \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros((n_pad, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m256\u001b[39m, \u001b[38;5;241m256\u001b[39m))\n\u001b[1;32m     12\u001b[0m         x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([x, zero_tensor], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m---> 14\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m onnx_model\u001b[38;5;241m.\u001b[39mrun(\u001b[43moutput_names\u001b[49m, {input_names[\u001b[38;5;241m0\u001b[39m]: x\u001b[38;5;241m.\u001b[39mnumpy()})[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     15\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m sci\u001b[38;5;241m.\u001b[39mspecial\u001b[38;5;241m.\u001b[39msoftmax(outputs[:config\u001b[38;5;241m.\u001b[39mBATCH_SIZE\u001b[38;5;241m-\u001b[39mn_pad, \u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m], axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     16\u001b[0m pred\u001b[38;5;241m.\u001b[39mappend(outputs)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'output_names' is not defined"
     ]
    }
   ],
   "source": [
    "predictions = []\n",
    "test_dataset = BirdDatasetInference(all_waves, transform=transforms_val)\n",
    "\n",
    "for ckpt in onnx_ckpt_list:\n",
    "    \n",
    "    # == init ONNX model ==\n",
    "    onnx_model = onnx.load(ckpt)\n",
    "    onnx_model_graph = onnx_model.graph\n",
    "    onnx_session = ort.InferenceSession(onnx_model.SerializeToString())\n",
    "    \n",
    "    # == create dataset & dataloader ==\n",
    "    test_loader = torch.utils.data.DataLoader(\n",
    "        test_dataset,\n",
    "        batch_size=config.BATCH_SIZE,\n",
    "        num_workers=config.N_WORKERS,\n",
    "        shuffle=False,\n",
    "        drop_last=False\n",
    "    )\n",
    "    \n",
    "    predictions.append(predict(test_loader, onnx_session))\n",
    "    gc.collect()\n",
    "\n",
    "# predictions = np.mean(predictions, axis=0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
