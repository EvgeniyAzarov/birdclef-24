{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import gc\n",
    "import sys\n",
    "import cv2\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from glob import glob\n",
    "from tqdm.auto import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import librosa\n",
    "import scipy as sci\n",
    "import timm\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torchvision.models import efficientnet\n",
    "\n",
    "import pytorch_lightning as pl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torchvision\n",
    "import torch\n",
    "import pytorch_lightning as pl\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "import math\n",
    "import librosa\n",
    "import cv2\n",
    "import torch.nn as nn\n",
    "from metric import score\n",
    "import albumentations as A\n",
    "import numpy as np\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import torchaudio\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "    # == global config ==\n",
    "    SEED = 42 # random seed\n",
    "    DEVICE = 'cuda'  # device to be used\n",
    "    MIXED_PRECISION = False  # whether to use mixed-16 precision\n",
    "    # OUTPUT_DIR = '/kaggle/working/'  # output folder\n",
    "    OUTPUT_DIR = '/data/yaz/birdclef24/out/'  # output folder\n",
    "    CKPT_ROOT = '/data/yaz/birdclef24/baselinev2/out/'\n",
    "\n",
    "    use_cache = False\n",
    "    \n",
    "    # == data config ==\n",
    "    # DATA_ROOT = '/kaggle/input/birdclef-2024'  # root folder\n",
    "    DATA_ROOT = '/data/yaz/birdclef24/data'  # root folder\n",
    "    # PREPROCESSED_DATA_ROOT = '/kaggle/input/birdclef24-spectrograms-via-cupy'\n",
    "    # PREPROCESSED_DATA_ROOT = '/data/yaz/birdclef24/data/specs'\n",
    "    # LOAD_DATA = True  # whether to load data from pre-processed dataset\n",
    "\n",
    "    image_size = 256\n",
    "\n",
    "    SR = 32000  # sample rate\n",
    "    mel_spec_params = {\n",
    "        \"sample_rate\": 32000,\n",
    "        \"n_mels\": 128,\n",
    "        \"f_min\": 20,\n",
    "        \"f_max\": 16000,\n",
    "        \"n_fft\": 2048,\n",
    "        \"hop_length\": 512,\n",
    "        \"normalized\": True,\n",
    "        \"center\" : True,\n",
    "        \"pad_mode\" : \"constant\",\n",
    "        \"norm\" : \"slaney\",\n",
    "        \"onesided\" : True,\n",
    "        \"mel_scale\" : \"slaney\"\n",
    "    }\n",
    "\n",
    "    top_db = 80 \n",
    "\n",
    "    train_period = 5\n",
    "    val_period = 5\n",
    "    secondary_coef = 1.0\n",
    "    train_duration = train_period * mel_spec_params[\"sample_rate\"]\n",
    "    val_duration = val_period * mel_spec_params[\"sample_rate\"]\n",
    "\n",
    "    # == model config ==\n",
    "    MODEL_TYPE = 'efficientnet_b0'  # model type\n",
    "    \n",
    "    # == dataset config ==\n",
    "    # BATCH_SIZE = 128  # batch size of each step\n",
    "    BATCH_SIZE = 256 # batch size of each step\n",
    "    N_WORKERS = 4  # number of workers\n",
    "    # N_WORKERS = 0  # number of workers\n",
    "    \n",
    "    # == AUG ==\n",
    "    USE_XYMASKING = True  # whether use XYMasking\n",
    "    \n",
    "    # == training config ==\n",
    "    N_FOLDS = 4  # n fold\n",
    "    EPOCHS = 30  # max epochs\n",
    "    LR = 1e-3  # learning rate\n",
    "    WEIGHT_DECAY = 1e-5  # weight decay of optimizer\n",
    "\n",
    "    \n",
    "    # == other config ==\n",
    "    VISUALIZE = False # whether to visualize data and batch\n",
    "\n",
    "\n",
    "config = Config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fix seed\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "42"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('fix seed')\n",
    "pl.seed_everything(config.SEED, workers=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(config.DATA_ROOT + '/train_metadata.csv')\n",
    "df[\"path\"] = config.DATA_ROOT + \"/train_audio/\" + df[\"filename\"]\n",
    "df[\"rating\"] = np.clip(df[\"rating\"] / df[\"rating\"].max(), 0.1, 1.0)\n",
    "\n",
    "skf = StratifiedKFold(n_splits=config.N_FOLDS, random_state=config.SEED, shuffle=True)\n",
    "df['fold'] = -1\n",
    "for ifold, (train_idx, val_idx) in enumerate(skf.split(X=df, y=df[\"primary_label\"].values)):\n",
    "    df.loc[val_idx, 'fold'] = ifold\n",
    "\n",
    "sub = pd.read_csv(config.DATA_ROOT + \"/sample_submission.csv\")\n",
    "target_columns = sub.columns.tolist()[1:]\n",
    "label_list = target_columns\n",
    "num_classes = len(target_columns)\n",
    "bird2id = {b: i for i, b in enumerate(target_columns)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_melspec(X, eps=1e-6):\n",
    "    mean = X.mean((1, 2), keepdim=True)\n",
    "    std = X.std((1, 2), keepdim=True)\n",
    "    Xstd = (X - mean) / (std + eps)\n",
    "\n",
    "    norm_min, norm_max = (\n",
    "        Xstd.min(-1)[0].min(-1)[0],\n",
    "        Xstd.max(-1)[0].max(-1)[0],\n",
    "    )\n",
    "    fix_ind = (norm_max - norm_min) > eps * torch.ones_like(\n",
    "        (norm_max - norm_min)\n",
    "    )\n",
    "    V = torch.zeros_like(Xstd)\n",
    "    if fix_ind.sum():\n",
    "        V_fix = Xstd[fix_ind]\n",
    "        norm_max_fix = norm_max[fix_ind, None, None]\n",
    "        norm_min_fix = norm_min[fix_ind, None, None]\n",
    "        V_fix = torch.max(\n",
    "            torch.min(V_fix, norm_max_fix),\n",
    "            norm_min_fix,\n",
    "        )\n",
    "        V_fix = (V_fix - norm_min_fix) / (norm_max_fix - norm_min_fix)\n",
    "        V[fix_ind] = V_fix\n",
    "    return V\n",
    "\n",
    "\n",
    "def read_wav(path):\n",
    "    wav, org_sr = torchaudio.load(path, normalize=True)\n",
    "    wav = torchaudio.functional.resample(wav, orig_freq=org_sr, new_freq=config.SR)\n",
    "    return wav\n",
    "\n",
    "\n",
    "def crop_start_wav(wav, duration_):\n",
    "    while wav.size(-1) < duration_:\n",
    "        wav = torch.cat([wav, wav], dim=1)\n",
    "    wav = wav[:, :duration_]\n",
    "    return wav"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "transforms_val = A.Compose([\n",
    "    A.Resize(config.image_size, config.image_size),\n",
    "    A.Normalize()\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:01<00:00,  6.09it/s]\n"
     ]
    }
   ],
   "source": [
    "all_waves = []\n",
    "\n",
    "# https://www.kaggle.com/code/markwijkhuizen/birdclef-2024-efficientvit-inference\n",
    "if len(glob(f'{config.DATA_ROOT}/test_soundscapes/*.ogg')) > 0:\n",
    "    ogg_file_paths = glob(f'{config.DATA_ROOT}/test_soundscapes/*.ogg')\n",
    "else:\n",
    "    ogg_file_paths = sorted(glob(f'{config.DATA_ROOT}/unlabeled_soundscapes/*.ogg'))[:10]\n",
    "\n",
    "for i, file_path in tqdm(enumerate(ogg_file_paths), total=len(ogg_file_paths)):\n",
    "    row_id = re.search(r'/([^/]+)\\.ogg$', file_path).group(1)  # filename\n",
    "    audio_data = read_wav(file_path)\n",
    "\n",
    "    SR = config.SR\n",
    "    SEGMENT_DURATION = 5  # seconds\n",
    "    SEGMENT_SAMPLES = SR * SEGMENT_DURATION\n",
    "\n",
    "    # Iterate over the segments\n",
    "    for j in range(48):\n",
    "        start_idx = SR * SEGMENT_DURATION * j\n",
    "        end_idx = SR * SEGMENT_DURATION * (j + 1)\n",
    "        wave = audio_data[:, start_idx:end_idx]\n",
    "        \n",
    "        # Check if the wave is shorter than the required length\n",
    "        if wave.size(1) < SEGMENT_SAMPLES:\n",
    "            # Pad the wave with zeros\n",
    "            padding = SEGMENT_SAMPLES - wave.size(1)\n",
    "            wave = torch.nn.functional.pad(wave, (0, padding), \"constant\", 0)\n",
    "\n",
    "        all_waves.append(wave) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.044696386363636376\n",
      "0.09015308333333337\n",
      "0.1356099090909091\n",
      "0.1810659696969697\n",
      "0.2265226515151514\n",
      "0.27197946212121177\n",
      "0.3174357651515142\n",
      "0.36289228030302884\n",
      "0.407664734848483\n",
      "0.452509227272725\n",
      "0.497227431818179\n",
      "0.5419158030303007\n",
      "0.5865928106060586\n",
      "0.6309230606060592\n",
      "0.6749245303030291\n",
      "0.7196155833333322\n",
      "0.7650720681818175\n",
      "0.8105288939393936\n",
      "0.8559851969696965\n",
      "0.9006658863636359\n",
      "0.9453518333333333\n",
      "0.990808431818182\n"
     ]
    }
   ],
   "source": [
    "ogg_file_paths = sorted(glob(f'{config.DATA_ROOT}/unlabeled_soundscapes/*.ogg'))\n",
    "\n",
    "sum_len = 0\n",
    "i = 0\n",
    "while sum_len < 1100 * 4 * 60:\n",
    "    wav = read_wav(ogg_file_paths[i])\n",
    "    sum_len += wav.shape[-1] / config.SR\n",
    "    i += 1\n",
    "\n",
    "    if i % 50 == 0:\n",
    "        print(sum_len / (1100 * 4 * 60))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1111"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BirdDatasetInference(torch.utils.data.Dataset):\n",
    "    def __init__(self, wavs, transform=None, add_secondary_labels=False, mode=None, use_cache=config.use_cache):\n",
    "        self.wavs = wavs\n",
    "        self.processed = [False for _ in range(len(self.wavs))]\n",
    "        self.bird2id = bird2id\n",
    "        self.num_classes = num_classes\n",
    "        self.secondary_coef = config.secondary_coef\n",
    "        self.add_secondary_labels = add_secondary_labels\n",
    "        self.mel_transform = torchaudio.transforms.MelSpectrogram(**config.mel_spec_params)\n",
    "        self.db_transform = torchaudio.transforms.AmplitudeToDB(stype='power', top_db=config.top_db)\n",
    "        self.transform = transform\n",
    "        self.mode = mode\n",
    "        self.use_cache = use_cache\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.wavs)\n",
    "\n",
    "    def prepare_spec(self, wav):\n",
    "        mel_spectrogram = normalize_melspec(self.db_transform(self.mel_transform(wav)))\n",
    "        mel_spectrogram = mel_spectrogram * 255\n",
    "        mel_spectrogram = mel_spectrogram.expand(3, -1, -1).permute(1, 2, 0).numpy()\n",
    "        return mel_spectrogram\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        if not self.processed[idx]:\n",
    "            spec = self.prepare_spec(self.wavs[idx])\n",
    "\n",
    "            if self.transform is not None:\n",
    "                res = self.transform(image=spec)\n",
    "                spec = res['image'].astype(np.float32)\n",
    "            else:\n",
    "                spec = spec.astype(np.float32)\n",
    "\n",
    "            spec = spec.transpose(2, 0, 1)\n",
    "\n",
    "            if self.use_cache:\n",
    "                self.wavs[idx] = spec\n",
    "                self.processed[idx] = True\n",
    "        else:\n",
    "            spec = self.wavs[idx]\n",
    "\n",
    "        return {\"spec\": spec}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EffNet(nn.Module):\n",
    "    def __init__(self, model_name=config.MODEL_TYPE, num_classes=None) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.model = timm.create_model(\n",
    "            model_name, \n",
    "            pretrained=True, \n",
    "            in_chans=3, \n",
    "            num_classes=num_classes\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # [B, W, H] -> [B, 1, W, H]\n",
    "        # x = x.unsqueeze(1)\n",
    "        x = self.model(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FocalLossBCE():\n",
    "    pass\n",
    "\n",
    "class BirdModel(pl.LightningModule):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        # == backbone ==\n",
    "        self.backbone = EffNet(num_classes=num_classes)\n",
    "        \n",
    "        # == loss function ==\n",
    "        # self.loss_fn = nn.CrossEntropyLoss()\n",
    "        self.loss_fn = FocalLossBCE() \n",
    "        \n",
    "        # == record ==\n",
    "        self.validation_step_outputs = []\n",
    "        \n",
    "    def forward(self, images):\n",
    "        return self.backbone(images)\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        \n",
    "        # == define optimizer ==\n",
    "        model_optimizer = torch.optim.Adam(\n",
    "            filter(lambda p: p.requires_grad, self.parameters()),\n",
    "            lr=config.LR,\n",
    "            weight_decay=config.WEIGHT_DECAY\n",
    "        )\n",
    "        \n",
    "        lr_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "            model_optimizer,\n",
    "            T_max=config.EPOCHS,\n",
    "            eta_min=1e-6,\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'optimizer': model_optimizer,\n",
    "            'lr_scheduler': {\n",
    "                'scheduler': lr_scheduler,\n",
    "                'interval': 'epoch',\n",
    "                'monitor': 'val_loss',\n",
    "                'frequency': 1\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    @staticmethod\n",
    "    def mixup(data, targets, alpha):\n",
    "        indices = torch.randperm(data.size(0))\n",
    "        data2 = data[indices]\n",
    "        targets2 = targets[indices]\n",
    "\n",
    "        lam = torch.tensor([np.random.beta(alpha, alpha)], device=data.device)\n",
    "        data = data * lam + data2 * (1 - lam)\n",
    "        targets = targets * lam + targets2 * (1 - lam)\n",
    "\n",
    "        return data, targets\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        \n",
    "        # == obtain input and target ==\n",
    "        image, target = batch[\"spec\"], batch[\"target\"]\n",
    "        image, target = self.mixup(image, target, 0.5)\n",
    "        \n",
    "        # == pred ==\n",
    "        y_pred = self(image)\n",
    "        \n",
    "        # == compute loss ==\n",
    "        train_loss = self.loss_fn(y_pred, target)\n",
    "        \n",
    "        # == record ==\n",
    "        self.log('train_loss', train_loss, prog_bar=True, on_step=True, on_epoch=True)\n",
    "        \n",
    "        return train_loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        \n",
    "        # == obtain input and target ==\n",
    "        image, target = batch['spec'], batch['target']\n",
    "        image = image.to(self.device)\n",
    "        target = target.to(self.device)\n",
    "        \n",
    "        # == pred ==\n",
    "        with torch.no_grad():\n",
    "            y_pred = self(image)\n",
    "            \n",
    "        self.validation_step_outputs.append({\"logits\": y_pred, \"targets\": target})\n",
    "        \n",
    "    def train_dataloader(self):\n",
    "        return self._train_dataloader\n",
    "\n",
    "    def validation_dataloader(self):\n",
    "        return self._validation_dataloader\n",
    "    \n",
    "    def on_validation_epoch_end(self):\n",
    "        \n",
    "        # = merge batch data =\n",
    "        outputs = self.validation_step_outputs\n",
    "        \n",
    "        output_val = nn.Softmax(dim=1)(torch.cat([x['logits'] for x in outputs], dim=0)).cpu().detach()\n",
    "        target_val = torch.cat([x['targets'] for x in outputs], dim=0).cpu().detach()\n",
    "        \n",
    "        # = compute validation loss =\n",
    "        val_loss = self.loss_fn(output_val, target_val)\n",
    "        \n",
    "        # target to one-hot\n",
    "        # target_val = torch.nn.functional.one_hot(target_val, num_classes)\n",
    "        \n",
    "        # = val with ROC AUC =\n",
    "        gt_df = pd.DataFrame(target_val.numpy().astype(np.float32), columns=label_list)\n",
    "        pred_df = pd.DataFrame(output_val.numpy().astype(np.float32), columns=label_list)\n",
    "        \n",
    "        gt_df['id'] = [f'id_{i}' for i in range(len(gt_df))]\n",
    "        pred_df['id'] = [f'id_{i}' for i in range(len(pred_df))]\n",
    "        \n",
    "        val_score = score(gt_df, pred_df, row_id_column_name='id')\n",
    "        \n",
    "        self.log(\"val_score\", val_score, on_epoch=True)\n",
    "        self.log(\"val_loss\", val_loss, on_epoch=True)\n",
    "        \n",
    "        # clear validation outputs\n",
    "        self.validation_step_outputs = list()\n",
    "        \n",
    "        return {'val_loss': val_loss, 'val_score': val_score}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt_list = [f'{config.CKPT_ROOT}/fold_{i}.ckpt' for i in range(config.N_FOLDS)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_tensor = torch.randn(config.BATCH_SIZE, 3, 256, 256)  # input shape\n",
    "input_names = ['x']\n",
    "output_names = ['output']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/efficientnet_b0.ra_in1k)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:timm.models._hub:[timm/efficientnet_b0.ra_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.\n",
      "INFO:timm.models._builder:Missing keys (classifier.weight, classifier.bias) discovered while loading pretrained weights. This is expected if model is being adapted.\n",
      "INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/efficientnet_b0.ra_in1k)\n",
      "INFO:timm.models._hub:[timm/efficientnet_b0.ra_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.\n",
      "INFO:timm.models._builder:Missing keys (classifier.weight, classifier.bias) discovered while loading pretrained weights. This is expected if model is being adapted.\n",
      "INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/efficientnet_b0.ra_in1k)\n",
      "INFO:timm.models._hub:[timm/efficientnet_b0.ra_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.\n",
      "INFO:timm.models._builder:Missing keys (classifier.weight, classifier.bias) discovered while loading pretrained weights. This is expected if model is being adapted.\n",
      "INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/efficientnet_b0.ra_in1k)\n",
      "INFO:timm.models._hub:[timm/efficientnet_b0.ra_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.\n",
      "INFO:timm.models._builder:Missing keys (classifier.weight, classifier.bias) discovered while loading pretrained weights. This is expected if model is being adapted.\n"
     ]
    }
   ],
   "source": [
    "onnx_ckpt_list = list()\n",
    "for ckpt_path in ckpt_list:\n",
    "    ckpt_name = os.path.basename(ckpt_path).split('.')[0]\n",
    "    # == init model ==\n",
    "    bird_model = BirdModel()\n",
    "    \n",
    "    # == load ckpt ==\n",
    "    weights = torch.load(ckpt_path, map_location=torch.device('cpu'))['state_dict']\n",
    "    bird_model.load_state_dict(weights)\n",
    "    bird_model.eval()\n",
    "    \n",
    "    # == convert to onnx ==\n",
    "    torch.onnx.export(bird_model.backbone, input_tensor, f\"{ckpt_name}.onnx\", verbose=False, input_names=input_names, output_names=output_names)\n",
    "    \n",
    "    onnx_ckpt_list.append(f\"{ckpt_name}.onnx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(data_loader, onnx_model):\n",
    "    pred = []\n",
    "    for batch in tqdm(data_loader):\n",
    "        with torch.no_grad():\n",
    "            x = batch['spec']\n",
    "            n_pad = 0\n",
    "            \n",
    "            # == make sure the batch_size equal to setting\n",
    "            if x.shape[0] < config.BATCH_SIZE:\n",
    "                n_pad = config.BATCH_SIZE - x.shape[0]\n",
    "                zero_tensor = torch.zeros((n_pad, 3, 256, 256))\n",
    "                x = torch.cat([x, zero_tensor], dim=0)\n",
    "            \n",
    "            outputs = onnx_model.run(output_names, {input_names[0]: x.numpy()})[0]\n",
    "            outputs = sci.special.softmax(outputs[:config.BATCH_SIZE-n_pad, ...], axis=1)\n",
    "        pred.append(outputs)\n",
    "    \n",
    "    return np.concatenate(pred, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnx\n",
    "import onnxruntime as ort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/yaz/birdclef24/.venv/lib/python3.10/site-packages/torchaudio/transforms/_transforms.py:580: UserWarning: Argument 'onesided' has been deprecated and has no influence on the behavior of this module.\n",
      "  warnings.warn(\n",
      "100%|██████████| 2/2 [00:08<00:00,  4.15s/it]\n",
      "100%|██████████| 2/2 [00:10<00:00,  5.46s/it]\n",
      "100%|██████████| 2/2 [00:09<00:00,  4.58s/it]\n",
      "100%|██████████| 2/2 [00:10<00:00,  5.08s/it]\n"
     ]
    }
   ],
   "source": [
    "predictions = []\n",
    "test_dataset = BirdDatasetInference(all_waves, transform=transforms_val)\n",
    "\n",
    "for ckpt in onnx_ckpt_list:\n",
    "    \n",
    "    # == init ONNX model ==\n",
    "    onnx_model = onnx.load(ckpt)\n",
    "    onnx_model_graph = onnx_model.graph\n",
    "    onnx_session = ort.InferenceSession(onnx_model.SerializeToString())\n",
    "    \n",
    "    # == create dataset & dataloader ==\n",
    "    test_loader = torch.utils.data.DataLoader(\n",
    "        test_dataset,\n",
    "        batch_size=config.BATCH_SIZE,\n",
    "        num_workers=config.N_WORKERS,\n",
    "        shuffle=False,\n",
    "        drop_last=False\n",
    "    )\n",
    "    \n",
    "    predictions.append(predict(test_loader, onnx_session))\n",
    "    gc.collect()\n",
    "\n",
    "predictions = np.mean(predictions, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submissionn shape: (3, 183)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>row_id</th>\n",
       "      <th>asbfly</th>\n",
       "      <th>ashdro1</th>\n",
       "      <th>ashpri1</th>\n",
       "      <th>ashwoo2</th>\n",
       "      <th>asikoe2</th>\n",
       "      <th>asiope1</th>\n",
       "      <th>aspfly1</th>\n",
       "      <th>aspswi1</th>\n",
       "      <th>barfly1</th>\n",
       "      <th>...</th>\n",
       "      <th>whbwoo2</th>\n",
       "      <th>whcbar1</th>\n",
       "      <th>whiter2</th>\n",
       "      <th>whrmun</th>\n",
       "      <th>whtkin2</th>\n",
       "      <th>woosan</th>\n",
       "      <th>wynlau1</th>\n",
       "      <th>yebbab1</th>\n",
       "      <th>yebbul3</th>\n",
       "      <th>zitcis1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>soundscape_1446779_5</td>\n",
       "      <td>0.002847</td>\n",
       "      <td>0.009961</td>\n",
       "      <td>0.000711</td>\n",
       "      <td>0.000421</td>\n",
       "      <td>0.002007</td>\n",
       "      <td>0.000405</td>\n",
       "      <td>0.000907</td>\n",
       "      <td>0.000797</td>\n",
       "      <td>0.001996</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001355</td>\n",
       "      <td>0.001477</td>\n",
       "      <td>0.002000</td>\n",
       "      <td>0.011780</td>\n",
       "      <td>0.013023</td>\n",
       "      <td>0.014849</td>\n",
       "      <td>0.000746</td>\n",
       "      <td>0.000425</td>\n",
       "      <td>0.000860</td>\n",
       "      <td>0.003528</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>soundscape_1446779_10</td>\n",
       "      <td>0.002157</td>\n",
       "      <td>0.017926</td>\n",
       "      <td>0.000827</td>\n",
       "      <td>0.000664</td>\n",
       "      <td>0.002602</td>\n",
       "      <td>0.000589</td>\n",
       "      <td>0.002117</td>\n",
       "      <td>0.000558</td>\n",
       "      <td>0.001529</td>\n",
       "      <td>...</td>\n",
       "      <td>0.003426</td>\n",
       "      <td>0.001482</td>\n",
       "      <td>0.002379</td>\n",
       "      <td>0.002172</td>\n",
       "      <td>0.017337</td>\n",
       "      <td>0.022887</td>\n",
       "      <td>0.001055</td>\n",
       "      <td>0.000544</td>\n",
       "      <td>0.001952</td>\n",
       "      <td>0.002735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>soundscape_1446779_15</td>\n",
       "      <td>0.001969</td>\n",
       "      <td>0.004878</td>\n",
       "      <td>0.001191</td>\n",
       "      <td>0.000752</td>\n",
       "      <td>0.001766</td>\n",
       "      <td>0.000684</td>\n",
       "      <td>0.001695</td>\n",
       "      <td>0.000666</td>\n",
       "      <td>0.001466</td>\n",
       "      <td>...</td>\n",
       "      <td>0.003006</td>\n",
       "      <td>0.001178</td>\n",
       "      <td>0.004148</td>\n",
       "      <td>0.001246</td>\n",
       "      <td>0.002667</td>\n",
       "      <td>0.032489</td>\n",
       "      <td>0.001007</td>\n",
       "      <td>0.001102</td>\n",
       "      <td>0.000795</td>\n",
       "      <td>0.003255</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 183 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  row_id    asbfly   ashdro1   ashpri1   ashwoo2   asikoe2  \\\n",
       "0   soundscape_1446779_5  0.002847  0.009961  0.000711  0.000421  0.002007   \n",
       "1  soundscape_1446779_10  0.002157  0.017926  0.000827  0.000664  0.002602   \n",
       "2  soundscape_1446779_15  0.001969  0.004878  0.001191  0.000752  0.001766   \n",
       "\n",
       "    asiope1   aspfly1   aspswi1   barfly1  ...   whbwoo2   whcbar1   whiter2  \\\n",
       "0  0.000405  0.000907  0.000797  0.001996  ...  0.001355  0.001477  0.002000   \n",
       "1  0.000589  0.002117  0.000558  0.001529  ...  0.003426  0.001482  0.002379   \n",
       "2  0.000684  0.001695  0.000666  0.001466  ...  0.003006  0.001178  0.004148   \n",
       "\n",
       "     whrmun   whtkin2    woosan   wynlau1   yebbab1   yebbul3   zitcis1  \n",
       "0  0.011780  0.013023  0.014849  0.000746  0.000425  0.000860  0.003528  \n",
       "1  0.002172  0.017337  0.022887  0.001055  0.000544  0.001952  0.002735  \n",
       "2  0.001246  0.002667  0.032489  0.001007  0.001102  0.000795  0.003255  \n",
       "\n",
       "[3 rows x 183 columns]"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sub_pred = pd.DataFrame(predictions, columns=label_list)\n",
    "# sub_id = pd.DataFrame({'row_id': list(all_bird_data.keys())})\n",
    "# sub = pd.concat([sub_id, sub_pred], axis=1)\n",
    "\n",
    "sub.iloc[:, 1:] = predictions[:sub.shape[0], :]\n",
    "\n",
    "sub.to_csv('submission.csv',index=False)\n",
    "print(f'Submissionn shape: {sub.shape}')\n",
    "sub.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
